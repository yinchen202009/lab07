{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab07\n",
    "\n",
    "The lab involves:\n",
    "1. Simulate files containing random DNA, protein, and binary data\n",
    "2. Compressing the data\n",
    "3. Comparing between different compressions and sequences\n",
    "4. Compressing real data\n",
    "5. Estimating compression of 1000 terabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulate files containing random DNA, protein, and binary data\n",
    "Using **np.random.choice**,  100 megabytes(8 bits/byte* 1024 bytes/kilobyte* 1024 kilobytes/megabyte* 100)of random data was generated containing 100%, 90%, 80%, 70%, 60%, and 50% zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#100 megabytes of random data containing 100% zeroes\n",
    "hundred= np.random.choice([0, 1], size=[800000000], replace=True, p=[1, 0.0])\n",
    "hundred = np.packbits(hundred)\n",
    "print(hundred)\n",
    "open(\"zeros_100p\", \"wb\").write(hundred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 2 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100 megabytes of random data containing 90% zeroes\n",
    "ninety= np.random.choice([0, 1], size=[800000000], replace=True, p=[0.9, 0.1])\n",
    "ninety = np.packbits(ninety)\n",
    "print(ninety)\n",
    "open(\"zeros_90p\", \"wb\").write(ninety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 52   4 161 ...  16   9  18]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100 megabytes of random data containing 80% zeroes\n",
    "eighty= np.random.choice([0, 1], size=[800000000], replace=True, p=[0.8, 0.2])\n",
    "eighty = np.packbits(eighty)\n",
    "print(eighty)\n",
    "open(\"zeros_80p\", \"wb\").write(eighty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  67 100 ...   2 128 174]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100 megabytes of random data containing 70% zeroes\n",
    "seventy= np.random.choice([0, 1], size=[800000000], replace=True, p=[0.7, 0.3])\n",
    "seventy = np.packbits(seventy)\n",
    "print(seventy)\n",
    "open(\"zeros_70p\", \"wb\").write(seventy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89 222  52 ...  38 220   5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100 megabytes of random data containing 60% zeroes\n",
    "sixty= np.random.choice([0, 1], size=[800000000], replace=True, p=[0.6, 0.4])\n",
    "sixty = np.packbits(sixty)\n",
    "print(sixty)\n",
    "open(\"zeros_60p\", \"wb\").write(sixty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  80  89 ...  45 199  48]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100 megabytes of random data containing 50% zeroes\n",
    "fifty = np.random.choice([0, 1], size=[800000000], replace=True, p=[0.5, 0.5])\n",
    "fifty = np.packbits(fifty)\n",
    "print(fifty)\n",
    "open(\"zeros_50p\", \"wb\").write(fifty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G' 'T' 'G' ... 'T' 'G' 'A']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100 megabytes of random DNA sequence\n",
    "dna = np.random.choice(['A', 'C', 'T', 'G'], size=100000000, replace=True, p=[0.25, 0.25, 0.25, 0.25])\n",
    "print(dna)\n",
    "open(\"nt_seq.fa\", \"w\").write(\"\".join(dna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' 'G' 'M' ... 'Y' 'A' 'L']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100 megabytes of random protein sequence\n",
    "pro = np.random.choice(['G','A','L','M','A','W','K','Q','E','S','P','V','I','C','Y','H','R','N','D','T'],\n",
    "                       size=[100000000], replace=True, p=[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
    "                                                          0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
    "                                                          0.05, 0.05, ])\n",
    "print(pro)\n",
    "open(\"pro_seq.fa\", \"w\").write(\"\".join(pro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compressing the data\n",
    "\n",
    "Each of the files generated above was compressed by running **gzip**, **bzip**, **pbzip2** and **ArithmeticCompress** in the terminal. \n",
    "\n",
    "The size of the input files, output files, and the time each command took to run was recorded in the table below:\n",
    "\n",
    "zeros_100p \n",
    "\n",
    "|Compression used | Command ran on terminal| Compressed File name | Compressed File Size(B) | Real Time used(s)|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | time gzip -k zeros_100p  | zeros_100p.gz | 97.1k | 0.688|\n",
    "| `bzip`    | time bzip2 -k zeros_100p  | zeros_100p1.bz2 | 113 | 1.010 |\n",
    "| `pbzip2`    | time pbzip2 -k zeros_100p |zeros_100p.bz2 |5.38k | 0.104 |\n",
    "| `ArithmeticCompress`  | time ArithmeticCompress zeros_100p zeros_100p.art |zeros_100p.art | 1.03k | 14.176|\n",
    "\n",
    "zeros_90p \n",
    "\n",
    "|Compression used | Command ran on terminal| Compressed File name | Compressed File Size(B) | Real Time used(s)|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | time gzip -k zeros_90p  | zeros_90p.gz | 56M | 18.070|\n",
    "| `bzip`    | time bzip2 -k zeros_90p  | zeros_90p1.bz2 | 58.3M | 10.159 |\n",
    "| `pbzip2`    | time pbzip2 -k zeros_90p |zeros_90p.bz2 |58.3M | 0.736 |\n",
    "| `ArithmeticCompress`  | time ArithmeticCompress zeros_90p zeros_90p.art |zeros_90p.art | 46.9M | 0.196|\n",
    "\n",
    "zeros_80p \n",
    "\n",
    "|Compression used | Command ran on terminal| Compressed File name | Compressed File Size(B) | Real Time used(s)|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | time gzip -k zeros_80p  | zeros_80p.gz | 77.4M | 12.989|\n",
    "| `bzip`    | time bzip2 -k zeros_80p  | zeros_80p1.bz2 | 82.6M | 11.598|\n",
    "| `pbzip2`    | time pbzip2 -k zeros_80p |zeros_80p.bz2 | 82.6M| 0.936 |\n",
    "| `ArithmeticCompress`  | time ArithmeticCompress zeros_80p zeros_80p.art |zeros_80p.art | 15.9M | 33.824|\n",
    "\n",
    "zeros_70p \n",
    "\n",
    "|Compression used | Command ran on terminal| Compressed File name | Compressed File Size(B) | Real Time used(s)|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | time gzip -k zeros_70p  | zeros_70p.gz | 89.3M | 6.185|\n",
    "| `bzip`    | time bzip2 -k zeros_70p  | zeros_70p1.bz2 | 95.1M | 13.198|\n",
    "| `pbzip2`    | time pbzip2 -k zeros_70p |zeros_70p.bz2 |95.1M | 1.113 |\n",
    "| `ArithmeticCompress`  | time ArithmeticCompress zeros_70p zeros_70p.art |zeros_70p.art | 88.1M | 37.687|\n",
    "\n",
    "zeros_60p \n",
    "\n",
    "|Compression used | Command ran on terminal| Compressed File name | Compressed File Size(B) | Real Time used(s)|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | time gzip -k zeros_60p  | zeros_60p.gz | 97.7M | 4.079|\n",
    "| `bzip`    | time bzip2 -k zeros_60p  | zeros_60p1.bz2 | 100M | 14.992 |\n",
    "| `pbzip2`    | time pbzip2 -k zeros_60p |zeros_60p.bz2 |100M | 1.308 |\n",
    "| `ArithmeticCompress`  | time ArithmeticCompress zeros_60p zeros_60p.art |zeros_60p.art | 97.1M | 39.078|\n",
    "\n",
    "zeros_50p \n",
    "\n",
    "|Compression used | Command ran on terminal| Compressed File name | Compressed File Size(B) | Real Time used(s)|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | time gzip -k zeros_50p  | zeros_50p.gz | 100M | 3.398|\n",
    "| `bzip`    | time bzip2 -k zeros_50p  | zeros_50p1.bz2 | 100M | 15.858 |\n",
    "| `pbzip2`    | time pbzip2 -k zeros_50p |zeros_50p.bz2 |100M | 1.388 |\n",
    "| `ArithmeticCompress`  | time ArithmeticCompress zeros_50p zeros_50p.art |zeros_50p.art | 100M| 39.414|\n",
    "\n",
    "nt_seq.fa\n",
    "\n",
    "|Compression used | Command ran on terminal| Compressed File name | Compressed File Size(B) | Real Time used(s)|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | time gzip -k nt_seq.fa  | nt_seq.fa.gz  | 29.2M | 12.149|\n",
    "| `bzip`    | time bzip2 -k nt_seq.fa  | nt_seq.fa1.bz2 | 27.3M | 9.458|\n",
    "| `pbzip2`    | time pbzip2 -k nt_seq.fa |nt_seq.fa.bz2 |27.3M | 0.690 |\n",
    "| `ArithmeticCompress`  | time ArithmeticCompress nt_seq.fa nt_seq.art |nt_seq.art | 25M| 22.216|\n",
    "\n",
    "pro_seq.fa\n",
    "\n",
    "|Compression used | Command ran on terminal| Compressed File name | Compressed File Size(B) | Real Time used(s)|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | time gzip -k pro_seq.fa  | pro_seq.fa.gz  | 59.5M | 4.492|\n",
    "| `bzip`    | time bzip2 -k pro_seq.fa  | pro_seq.fa1.bz2 | 54.2M | 9.915|\n",
    "| `pbzip2`    | time pbzip2 -k pro_seq.fa |pro_seq.fa.bz2 |54.2M | 0.786 |\n",
    "| `ArithmeticCompress`  | time ArithmeticCompress pro_seq.fa pro_seq.art |pro_seq.art | 52.8M| 28.315|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparing between different compressions and sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which algorithm achieves the best level of compression on each file type?\n",
    "`ArithmeticCompress` achieved the best level of compression on each file type, as shown in the table above. For each type of file, the file compressed by `ArithmeticCompress` is the smallest. Except for compression of _zeros_50p_, which may be because the data was in a uniform binary distribution. `ArithmeticCompress` is only more effective(archieves Shannon limit) if there is an algorithm to predict next character in the sequence.\n",
    "\n",
    "## Which algorithm is the fastest?\n",
    "`pbzip2` is the fastest, as it used the least real time, which is made up by sum of user and system time.\n",
    "\n",
    "## What is the difference between bzip2 and pbzip2? Do you expect one to be faster and why?\n",
    "Except for file zeros_100p, file compressed by `pbzip2` is always the same as compression of `bzip`. `pbzip2` compression is faster than `bzip` compression. I did not expect `pbzip2` to be faster, it is a version of `bzip2` that does parallel processing. \n",
    "\n",
    "## How does the level of compression change as the percentage of zeros increases?Why does this happen?\n",
    "As the percentage of zeroes increase, the level of compression becomes higher - the file with more zeroes can be compressed into a smaller file. The higher percentage of zeroes in the file, the lower percentages of numbers other than 0 exist in the same file, and the less the types of numbers exist in each file. It will be easier to compress because the more number of zeroes in the file(higher zero percentage means more zero will be \"selected\" to be in the file), the more likely is there to be a patten of zeroes generated, which is easier to store and increases the level of compression.\n",
    "\n",
    "## What is the minimum number of bits required to store a single DNA base?\n",
    "Bits needed = -log(number of possible nucleotides) in base 2\n",
    "There are 4 possible nucleotides: A, T, G, C\n",
    "Bits needed = log(4) in base 2\n",
    "            = 2 bits\n",
    "            \n",
    "## What is the minimum number of bits required to store an amino acid letter?            \n",
    "Bits needed = -log(number of possible amino acids) in base 2\n",
    "There are 20 possible nucleotides.\n",
    "Bits needed = log(20) in base 2\n",
    "            = 4.321 bits\n",
    "                        \n",
    "## In your tests, how many bits did gzip and bzip2 actually require to store your random DNA and protein sequences?\n",
    "For random DNA sequences, `gzip` needed 29.2MB and `bzip2` needed 27.3MB.\n",
    "For random protein sequences, `gzip` needed 59.5MB and `bzip2` needed 54.2MB.\n",
    "\n",
    "## Are gzip and bzip2 performing well on DNA and proteins?\n",
    "Yes. The initial DNA and protein sequences are 100MB each, and there is a great drop in the size of the file after compression. Both `gzip1` and `bzip` works better on DNA sequences because there is less number of variables (4 nucleotides thus 4 possiblities for each DNA letter) as compared to the 20 letters of amino acids in the random protein sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compressing real data\n",
    "\n",
    "The nucleic acid sequences of gp120 homologs from 10 different HIV isolates were discovered with the GSI Michael [here](https://www.hiv.lanl.gov/cgi-bin/NEWALIGN/align.cgi). They were being copied and pasted into a text file,\"a.txt\", and some modifications were done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15566"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any dashes due to alignments \n",
    "import re\n",
    "string = open('a.txt').read()\n",
    "new_str = re.sub('[^a-zA-Z0-9_.>]', '', string)\n",
    "open('b.fa', 'w').write(\"\".join(new_str))\n",
    "\n",
    "# A line break between was put between every nucleic acid sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing the multi-FASTA using gzip, bzip2, and arithmetic coding\n",
    "At priori, I expected there to be better compression than some of the random data(such as random protein sequence), because most of the homologs of HIV share similar sequences\n",
    "\n",
    "The table below shows the result of compression of the homolog:\n",
    "\n",
    "b.fa is 15.6 kB\n",
    "\n",
    "\n",
    "|Compression used | Command ran on terminal| Compressed File name | Compressed File Size(B) | Real Time used(s)|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | time gzip -k b.fa  | b.fa.gz | 3.63k | 0.005|\n",
    "| `bzip`    | time bzip2 -k b.fa  | b1.fa.bz2 | 3.52k | 0.008 |\n",
    "| `pbzip2`    | time pbzip2 -k b.fa |b.fa.bz2 | 3.52k | 0.011 |\n",
    "| `ArithmeticCompress`  | time ArithmeticCompress b.fa b.art | b.art | 5.15k | 0.016|\n",
    "\n",
    "\n",
    "### How does the compression ratio of this file compare to random data?\n",
    "Compression ratio = Inital file size/Compressed file size\n",
    "\n",
    "Compression ratio of  b.fa\n",
    "\n",
    "|Compression used| Inital file | Compressed file | Inital file size(B)| Compressed File Size(B)|Compression ratio|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | b.fa | b.fa.gz | 15.6k| 3.63k | 4.298|\n",
    "| `bzip`    | b.fa  | b1.fa.bz2 | 15.6k|3.52k | 4.432 |\n",
    "| `pbzip2`    | b.fa |b.fa.bz2 | 15.6k| 3.52k | 4.432 |\n",
    "| `ArithmeticCompress`  | b.fa | b.art | 15.6k | 5.15k| 3.029 |\n",
    "| Average Compression Ratio of b.fa ||||| 4.048|\n",
    "\n",
    "\n",
    "Compression ratio of nt_seq.fa\n",
    "\n",
    "|Compression used| Inital file | Compressed file | Inital file size(B)| Compressed File Size(B)|Compression ratio|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | nt_seq.fa | nt_seq.gz | 100M | 29.2M | 3.425|\n",
    "| `bzip`    | nt_seq.fa  | nt_seq.bz2 | 100M| 27.3M | 3.663 |\n",
    "| `pbzip2`    | nt_seq.fa |nt_seq.bz2 | 100M| 27.3M | 3.663 |\n",
    "| `ArithmeticCompress`  | b.fa | nt_seq.art | 100M | 25M | 4 |\n",
    "| Average Compression Ratio of nt_seq.fa ||||| 3.688|\n",
    "\n",
    "Compression ratio of pro_seq.fa\n",
    "\n",
    "|Compression used| Inital file | Compressed file | Inital file size(B)| Compressed File Size(B)|Compression ratio|\n",
    "|:-----------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
    "| `gzip`     | pro_seq.fa | pro_seq.gz | 100M | 59.5M | 1.681|\n",
    "| `bzip`    | pro_seq.fa  | pro_seq.bz2 | 100M| 54.2M | 1.845 |\n",
    "| `pbzip2`    | pro_seq.fa |pro_seq.bz2 | 100M| 54.2M | 1.845 |\n",
    "| `ArithmeticCompress`  | pro_seq.fa | pro_seq.art | 100M | 52.8M | 1.894 |\n",
    "| Average Compression Ratio of pro_seq.fa||||| 4.048|\n",
    "\n",
    "\n",
    "We can see that on average, the compression of the homolog fasta file is greater than the random DNA and protein sequences, for different compression computations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Estimating compression of 1000 terabytes\n",
    "\n",
    "* $50 per terabyte of hard disk space\n",
    "* My team willget a bonus this year equal to the amount of savings we’re able to generate by compressing the company’s data\n",
    "\n",
    "I propose using `bzip` for re-sequencing of genomes and plasmids that are very similar to each other. As we can see from the compression ratio table above, the compression ratio is the highest using `bzip` and `pbzip` when we are compressing **homologs**, a gene related to a second gene by descent from a common ancestral DNA sequence. This means that compressing similar sequences, `bzip` and `pbzip` gives the best compression results. `bzip` was prefered because `pbzip` used longer time. `bizip` can also be used if the protein sequences are similar. `bzip` uses **Burrows-Wheeler transformation**, which means that the repeated subtrings will show up as **repeated characters** after rotations of input string, causing it to be encoded and compressed much more effectively.\n",
    "\n",
    "I propose using `ArithmeticCompress` for compressing protein sequences and binary microscope images which, assuming follow the worst-case scenario of being completely random. As we can see from the compression ratio tables above, the compression ratio is the highest using `ArithmeticCompress` when we are compressing random sequences.\n",
    "\n",
    "## Estimate for the fraction of space saved using the above compression scheme\n",
    "For using `bzip` to compress re-sequencing of genomes and plasmids that are very similar to each other, according to the compression table, I can round down and assume the compression ratio of `bzip` for similar sequences is 4. Thus if 1000 terabytes were compressed to a file 4 times smaller to 250 terabytes, the company saved 750 terabytes. The 75percent reduction can generate 75percentx500 dollarsx365 days = 13687500 dollars bonus per year. \n",
    "\n",
    "For using `ArithmeticCompress` to compress protein sequences, according to the compression table for protein sequence(assuming follow the worst-case scenario of being completely random), I assume the compression ratio of `bzip` for random protein sequences is 4. Thus if 1000 terabytes were compressed to a file 4 times smaller to 250 terabytes, the company saved 750 terabytes. The 75percent reduction can generate 75x500 dollarsx365 days = 13687500 dollars bonus per year. \n",
    "\n",
    "Assuming the binary microscope images are completely random, compression may not be possible, modeled by the zeros_50p table above.\n",
    "\n",
    "\n",
    "Estimated total bonus this year= 13687500 dollars + 13687500 dollars \n",
    "                               = 27375000 dollars\n",
    "                               \n",
    "This estimation is **highly inaccurate** because the compression really depends on the type of sequences and files obtained by the company, of which the randomness or uniqueness of the sequences are not definite. Compressions of these files will also have _different compression ratios_ and generate _different type of files_ as required so the percentage of compression is indefinite as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
